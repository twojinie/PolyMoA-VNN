# vnn_model.py
import torch.nn as nn, torch
import math

class LinearMasked(nn.Linear):
    """weight âŠ™ mask (mask=0 ì€ ì™„ì „ ê³ ì •)."""
    def __init__(self, in_f: int, out_f: int, mask: torch.Tensor):
        super().__init__(in_f, out_f, bias=True)
        self.register_buffer('mask', mask.float())
        self._reset_with_mask()

    # -------- ì»¤ìŠ¤í…€ ì´ˆê¸°í™” --------
    def _reset_with_mask(self):
        active = self.mask.sum().item()      # 1ì˜ ê°œìˆ˜
        if active == 0:
            nn.init.zeros_(self.weight)
        else:
            fan_in = active / self.out_features
            bound  = 1. / math.sqrt(fan_in)
            nn.init.uniform_(self.weight, -bound, bound)
        if self.bias is not None:
            nn.init.zeros_(self.bias)

    def forward(self, x):
        return nn.functional.linear(x, self.weight * self.mask, self.bias)


class VisibleNN(nn.Module):
    """
    masks: [geneâ†’P5, P5â†’P4, â€¦, P1â†’output]  (len = 6)
    ë§ˆì§€ë§‰ maskëŠ” (1, |P1|) â€“ fully-connectedì—¬ë„ 1ë¡œ ì±„ì›Œì„œ ë„˜ê²¨ ì£¼ë©´ ë¨.
    """
    def __init__(self, masks):
        super().__init__()
        layers = []
        for m in masks[:-1]:                # geneâ†’P5 â€¦ P2â†’P1 ê¹Œì§€ ReLU
            layers += [LinearMasked(m.shape[1], m.shape[0], m), nn.ReLU()]
        # P1â†’output (binary logit)
        last = masks[-1]
        layers += [LinearMasked(last.shape[1], last.shape[0], last)]
        self.net = nn.Sequential(*layers)

    def forward(self, x):      # x: [B, N_gene]
        logit = self.net(x).squeeze(-1)   # [B]
        return logit             # BCEWithLogitsLoss ì— ë°”ë¡œ íˆ¬ì…

class VisibleNNWithSkip(VisibleNN):
    def __init__(self, masks, skip_mask):
        super().__init__(masks)

        in_f = skip_mask.shape[1]
        self.skip2out = LinearMasked(in_f, 1, skip_mask)
        self.alpha = nn.Parameter(torch.tensor(0.5))  # skip gate # learnable gate (sigmoid)

    def forward(self, x):
        skips, activs = [], []
        out = x

        # â‘  self.net ìˆœì°¨ ì‹¤í–‰
        for layer in self.net:
            out = layer(out)
            if isinstance(layer, LinearMasked):
                activs.append(out)      # ëª¨ë“  LinearMasked ì§í›„ ê²°ê³¼ ì €ì¥

        # activs êµ¬ì„±
        #  [0] geneâ†’P5  [1] P5â†’P4  [2] P4â†’P3  [3] P3â†’P2
        #  [4] P2â†’P1    [5] P1â†’output  â† ë§ˆì§€ë§‰ì€ ì´ë¯¸ logit
        logit_base = activs[-1]         # shape [B,1]
        top        = activs[-2]         # P1 activation
        skips      = activs[:-2]        # P5,P4,P3,P2 ì´ 4ê°œ
        skip_in    = torch.cat(skips, dim=1)   # shape [B, 3703]
        skip_out = self.skip2out(skip_in)
        gate = torch.sigmoid(self.alpha) # new

        # â‘¡ ìµœì¢… logit = ê¸°ë³¸ + skip ê²½ë¡œ
        logit = logit_base + gate * skip_out
        return logit.squeeze(-1)
    
# =====================================================
# ğŸ”¹ VNN with skip + gate regularization (Î» * Î±Â²)
# =====================================================
class VisibleNNWithSkipReg(VisibleNNWithSkip):
    def __init__(self, masks, skip_mask, reg_lambda=1e-3):
        super().__init__(masks, skip_mask)
        self.reg_lambda = reg_lambda

    def skip_reg_loss(self):
        """skip ê²Œì´íŠ¸ Î± ê·œì œí•­ (Î» * Î±Â²)"""
        gate = torch.sigmoid(self.alpha)
        return self.reg_lambda * (gate ** 2)

    def forward(self, x):
        # ê·¸ëŒ€ë¡œ VisibleNNWithSkip forward ì‚¬ìš©
        return super().forward(x)